stages:
  - collect_and_redact

# Set your values via CI/CD variables or override here.
variables:
  AWS_REGION: "us-west-2"                 # << your region
  INSTANCE_ID: "i-0123456789abcdef0"      # << target EC2 instance id
  LOG_PATHS: "/var/log/nginx /var/log/app" # << space-separated list of log roots on the instance
  S3_BUCKET: "my-log-ingest-bucket"       # << temp handoff bucket
  S3_PREFIX: "ssm-log-pulls"              # optional subfolder
  S3_SSE_KMS_KEY_ID: ""                   # e.g., alias/my-logs-key or a KMS key-id; leave blank for SSE-S3
  # Optional: set tags to select a specific runner (or remove this key entirely)
  # CI_RUNNER_TAGS: "aws,linux"

collect_redact_logs:
  image: public.ecr.aws/aws-cli/aws-cli:2
  stage: collect_and_redact
  tags:
    # If you want to target a specific runner, put its tags here (comma-separated above; split here)
    # Remove this 'tags' block if you don't need it.
    - ${CI_RUNNER_TAGS}
  before_script:
    - aws --version
    - test -n "$AWS_REGION"
    - test -n "$INSTANCE_ID"
    - test -n "$S3_BUCKET"
    - export TS="$(date +%Y%m%d-%H%M%S)"
    - export ARCHIVE_NAME="logs_${CI_PROJECT_NAME}_${CI_PIPELINE_ID}_${TS}.tar.gz"
    - export S3_KEY="${S3_PREFIX}/${CI_PROJECT_NAME}/${CI_PIPELINE_ID}/${ARCHIVE_NAME}"
  script:
    # a) & b) Run a non-interactive SSM command to archive logs and upload to S3 (from the instance)
    - |
      read -r -d '' REMOTE_SCRIPT <<'EOS'
      set -euo pipefail

      detect_pkg_mgr() {
        command -v yum >/dev/null 2>&1 && { echo yum; return; }
        command -v dnf >/dev/null 2>&1 && { echo dnf; return; }
        command -v apt-get >/dev/null 2>&1 && { echo apt-get; return; }
        command -v zypper >/dev/null 2>&1 && { echo zypper; return; }
        echo unknown
      }

      ensure_awscli() {
        command -v aws >/dev/null 2>&1 && return
        pm=$(detect_pkg_mgr)
        case "$pm" in
          yum|dnf) sudo $pm -y install unzip curl >/dev/null || true ;;
          apt-get) sudo apt-get update -y >/dev/null || true; sudo apt-get install -y curl unzip >/dev/null || true ;;
          zypper)  sudo zypper --non-interactive install curl unzip >/dev/null || true ;;
          *) echo "WARN: unknown package manager; trying CLI install anyway" ;;
        esac
        tmpdir=$(mktemp -d); cd "$tmpdir"
        curl -sSLo awscliv2.zip "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip"
        unzip -q awscliv2.zip
        sudo ./aws/install >/dev/null || true
      }

      ARCHIVE_NAME="${ARCHIVE_NAME}"
      LOG_PATHS="${LOG_PATHS}"
      AWS_REGION="${AWS_REGION}"
      S3_BUCKET="${S3_BUCKET}"
      S3_KEY="${S3_KEY}"
      S3_SSE_KMS_KEY_ID="${S3_SSE_KMS_KEY_ID}"

      ensure_awscli

      work=/tmp/gitlab-log-pull; mkdir -p "$work"; cd "$work"

      include=()
      for p in ${LOG_PATHS}; do
        [ -e "$p" ] && include+=("$p") || echo "Skipping missing $p" >&2
      done
      [ ${#include[@]} -gt 0 ] || { echo "No valid log paths found."; exit 2; }

      # Avoid scooping already-compressed rotations to keep size reasonable
      tar -czf "${ARCHIVE_NAME}" --warning=no-file-changed \
        --exclude='*.gz' --exclude='*.xz' --exclude='*.zip' --exclude='*.zstd' \
        "${include[@]}"

      if [ -n "${S3_SSE_KMS_KEY_ID}" ]; then
        aws s3 cp "${ARCHIVE_NAME}" "s3://${S3_BUCKET}/${S3_KEY}" \
          --region "${AWS_REGION}" --sse aws:kms --sse-kms-key-id "${S3_SSE_KMS_KEY_ID}"
      else
        aws s3 cp "${ARCHIVE_NAME}" "s3://${S3_BUCKET}/${S3_KEY}" \
          --region "${AWS_REGION}" --sse AES256
      fi

      echo "UPLOADED s3://${S3_BUCKET}/${S3_KEY}"
      EOS

      CMD_ID=$(aws ssm send-command \
        --document-name "AWS-RunShellScript" \
        --instance-ids "$INSTANCE_ID" \
        --region "$AWS_REGION" \
        --comment "Tar logs and push to S3 for pipeline ${CI_PIPELINE_ID}" \
        --parameters commands=["$REMOTE_SCRIPT"] \
        --query "Command.CommandId" --output text)

      echo "SSM CommandId: $CMD_ID"

      for _ in $(seq 1 60); do
        STATUS=$(aws ssm get-command-invocation \
          --command-id "$CMD_ID" --instance-id "$INSTANCE_ID" \
          --region "$AWS_REGION" --query "Status" --output text || true)
        echo "SSM status: $STATUS"
        case "$STATUS" in
          Success) break ;;
          Failed|Cancelled|TimedOut)
            echo "RunCommand failed with status: $STATUS"
            aws ssm get-command-invocation \
              --command-id "$CMD_ID" --instance-id "$INSTANCE_ID" \
              --region "$AWS_REGION" --query "StandardErrorContent" --output text || true
            exit 1
            ;;
        esac
        sleep 5
      done
      [ "$STATUS" = "Success" ] || { echo "RunCommand did not complete in time"; exit 1; }

    # c) Copy the archive from S3 to the local job workspace
    - aws s3 cp "s3://${S3_BUCKET}/${S3_KEY}" "./${ARCHIVE_NAME}" --region "$AWS_REGION"
    - mkdir -p raw_logs && tar -xzf "${ARCHIVE_NAME}" -C raw_logs

    # d) Redact (script in repo at scripts/redact.sh; includes sensible defaults)
    - chmod +x scripts/redact.sh
    - ./scripts/redact.sh raw_logs redacted_logs

    # e) Zip the redacted logs for artifacts
    - zip -rq "redacted_logs_${CI_PIPELINE_ID}.zip" redacted_logs

  artifacts:
    name: "redacted-logs-$CI_PIPELINE_ID"
    when: always
    expire_in: 14 days
    paths:
      - "redacted_logs_${CI_PIPELINE_ID}.zip"
    # If you ALSO want the raw archive and browsable folder, uncomment:
    #   - "${ARCHIVE_NAME}"
    #   - "redacted_logs/"

  rules:
    - if: $CI_COMMIT_BRANCH        # run on branches
    - if: $CI_PIPELINE_SOURCE == "schedule"
